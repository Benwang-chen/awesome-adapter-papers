   

- [Adaper-Papers](#adaper-papers)
  - [Introduction](#introduction)
  - [Paper lists](#paper-lists)
    - [Adapter based paradigm](#adapter-based-paradigm)
    - [other paradigm](#other-paradigm)




# Adaper-Papers


## Introduction
Due to the rise of pre-training models in the natural language field, large-scale pre-training models require transfer learning on different tasks. Compared with traditional fine-tuning methods, adapter based tuning is an efficient parameter transfer method. Here is a list of the latest efficient migration paradigms.


---
## Paper lists
### Adapter based paradigm

1. ICML-19-Parameter-Efficient Transfer Learning for NLP[PDF](http://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf)
2. ACL-21-Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks[PDF](https://arxiv.org/pdf/2106.04489.pdf)



### other paradigm
1. ACL-2021-Parameter-Efficient Transfer Learning with Diff Pruning[PDF](https://arxiv.org/pdf/2012.07463.pdf)
2. ACL-2021-Masking as an Efficient Alternative to Finetuning for Pretrained Language Models[PDF](https://aclanthology.org/2020.emnlp-main.174.pdf)
3. 

